{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8fa137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br/>\n",
    "<img src=\"images/cd-logo-blue-600x600.png\" alt=\"\" width=\"130px\" align=\"left\"/>\n",
    "<img src=\"images/cd-logo-blue-600x600.png\" alt=\"\" width=\"130px\" align=\"right\"/>\n",
    "<div align=\"center\">\n",
    "<h2>Bootcamp Data Science - Módulo 3</h2><br/>\n",
    "<h1>Boosting</h1>\n",
    "<br/><br/>\n",
    "    <b>Instructor Principal:</b> Patricio Olivares polivares@codingdojo.cl <br/>\n",
    "    <b>Instructor Asistente:</b> Jesús Ortiz jortiz@codingdojo.cl<br/><br/>\n",
    "    <b>Coding Dojo</b>\n",
    "</div>\n",
    "<br>\n",
    "Fuente: \"Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ee303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9ced8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que Bagging, el **Boosting** se encuentra en la categoría de métodos de **aprendizaje ensamblado**.\n",
    "- Bagging entrena un mismo modelo con **diferentes subsets de datos de entrenamiento**. (ej. Random Forest)\n",
    "- Boosting son aquellos métodos enfocados en **combinar** múltiples modelos débiles en un único modelo más robusto.\n",
    "- Otra diferencia radica en que bagging realiza entrenamientos de sus modelos en **paralelo**, mientras que boosting entrena sus modelos de manera **secuencial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f16c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bagging**\n",
    "<img src=\"images/bagging.png\" alt=\"\" width=\"500px\"/>\n",
    "\n",
    "**Boosting**\n",
    "<img src=\"images/boosting.png\" alt=\"\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddeb3db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe792b",
   "metadata": {},
   "source": [
    "- Cada predictor corrige al predictor antecesor poniendo mayor atención a las instancias mal clasificadas.\n",
    "- AdaBoost incrementa el peso relativo de los datos de entrenamiento mal clasificados por el predictor anterior (mayor importancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f548a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer_data = load_breast_cancer(as_frame=True) \n",
    "\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=cancer_data.feature_names)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e752237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9766081871345029\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "adb = AdaBoostClassifier(LogisticRegression(), n_estimators=100, random_state=42)\n",
    "adb.fit(X_train, y_train)\n",
    "print('Test score:', adb.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', adb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbbc02",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7caf4a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A diferencia de AdaBoost, Gradient Boosting permite que cada predictor se ajuste a los **errores residuales** del predictor anterior.\n",
    "- Ojo: Si bien, teóricamente es posible usar otros modelos con Gradient Boosting, la biblioteca de Scikit-Learn solo permite ser utilizada con Árboles de Decisión como predictor base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6932ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9590643274853801\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "print('Test score:', gb.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9c67a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# XGBoost vs LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b521c29",
   "metadata": {},
   "source": [
    "- La popularidad de Gradient Boosting ha generado múltiples implementaciones dirigidas a optimizar el cálculo de entrenamiento.\n",
    "- Dos implementaciones populares de Gradient Boosting en esta línea son **XGBoost** y **LightGBM**\n",
    "- **XGBoost** (eXtreme Gradient Boosting) es un algoritmo de boosting para árboles de decisión que aplica **crecimiento por nivel** (level-wise growth), el cual prioriza el crecimiento **horizontal** de los árboles\n",
    "- **LightGBM** (Light Gradient Boosting Machine) es un algoritmo de boosting para árboles de decisión que aplica **crecimiento por hojas** (leaf-wise tree growt), el cual prioriza el crecimiento **vertical** de los árboles.\n",
    "- Tanto XGBoost como LightGBM tienen sus propias bibliotecas, por lo que es necesario instalarlas previo uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6404f0e",
   "metadata": {},
   "source": [
    "### Crecimiento por nivel vs crecimiento por hojas\n",
    "<img src=\"images/lightgb_xgboostm.png\" alt=\"\" width=\"900px\"/>\n",
    "<br>\n",
    "Fuente https://felipesulser.github.io/ashrae-kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea5b5ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:19:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test score: 0.9824561403508771\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "CPU times: user 541 ms, sys: 0 ns, total: 541 ms\n",
      "Wall time: 52.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost (pip install xgboost)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "print('Test score:', xgb.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dd92346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9590643274853801\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# LightGBM (pip install lightgbm)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgbm.fit(X_train, y_train)\n",
    "print('Test score:', lgbm.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', lgbm.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba661a96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actividad 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847f5ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Utilice nuevamente el dataset California Housing presente en los dataset de Scikit-Learn (enlace [aquí](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)) y utilice los algoritmos de boosting para regresión de datos. \n",
    "- Compare los resultados obtenidos para cada algoritmo utilizando distintas métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a0a3261",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "#codigo extra, para que imagenes de matplotlib\n",
    "#estén centradas en las diapositivas, ejecutar antes de lanzar los ejemplos."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
